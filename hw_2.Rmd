---
title: "Suburbs in Boston Housing Prices"
subtitle: "HW II. Programming in R. MSc Statistics for Data Science."
author:
  - "Kendal Raymond William Smith <100494805@alumnos.uc3m.es>"
  - "Germán Blanco Blanco <100441287@alumnos.uc3m.es>"
date: '2022-10-29'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# WARNING

The dataset used in this exercise has an ethical/racist problem, as reported in [Carlisle](https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8).
It is used here together with this warning, to highlight our awareness about ethical issues in data science.
Furthermore, only variables that brought no concerns will be used in this study.

## Determination of Pupil to Teach Ration by Demographics in Boston Housing Dataset

In the [Boston Housing Dataset](https://www.kaggle.com/datasets/vikrishnan/boston-house-prices), there is one demographic variable that calls our attention.
This is the pupil to teacher ratio.

One of us lives in Paracuellos, a suburban town near Madrid, where it seems that every family has either more than two kids or several dogs.
It is interesting to see how people in need of space, but of different cultures, income levels and family structures have all gathered here.
Unsurprisingly, given the huge growth in a short period of years, one of the main problems of this town was low number of schools and teachers.
This study will look at a (otherwise controversial) dataset in the US, and try to identify the main factors for moving to such neighborhoods.

### Introduction

The goal of this work is to use several of the variables in the Boston Housing dataset and use them to classify the Pupil to Teacher ration variable.
We can see in its histogram that the variable has a big lump around 20.
These could be the houses that belong to the group that we want to spot (suburban neigborhoods with many pupils per teacher).

```{r}
boston <- read.csv("boston_house_prices.csv")
hist(boston$PTRATIO)
```

The median (19.05) of this variable is close to the point just below this lump around 20.
So we are going to select the rounded value of the median (19) as the point that splits the data and create a categorical variable for the classification.

The variables that we will use for input are the following:

Input features in order:  
1) ZN: proportion of residential land zoned for lots over 25,000 sq.ft.  
2) INDUS: proportion of non-retail business acres per town  
3) RM: average number of rooms per dwelling  
4) AGE: proportion of owner-occupied units built prior to 1940  
5) DIS: weighted distances to five Boston employment centres  
6) RAD: index of accessibility to radial highways  
7) TAX: full-value property-tax rate per $10,000 [$/10k]  
8) MEDV: Median value of owner-occupied homes in $1000's [k$]

Output variable:
9) PTRATIO: pupil-teacher ratio by town  

```{r}
boston_work <- boston[, c('ZN','INDUS','RM','AGE','DIS','RAD','TAX','MEDV')]
boston_work$y <- as.factor(ifelse(boston$PTRATIO > 19, "suburb", "city"))
```

### Training vs. Test Split (Caret)

We are going to use the caret package for training.
We also set random seed in order to make results reproducible.

```{r echo=TRUE, include=TRUE, warning=FALSE, message=FALSE}
if (!require("caret")) install.packages("caret")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("caTools")) install.packages("caTools")
library(caret) # Misc functions for training and plotting classification and regression models.
library(ggplot2) # ggplot2 is a system for declaratively creating graphics, based on The Grammar 
# of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical
# primitives to use, and it takes care of the details.
library(caTools)
random_seed<-7777777
set.seed(random_seed)
```

Let us start by dividing this dataset in a standard 70-30 split for training and testing.

```{r}
test_split <- createDataPartition(y = boston_work$y, p=0.7, list=FALSE)
training_set <- boston_work[test_split,]
test_set <- boston_work[-test_split,]
```

In order to tune the model parameters, we are going to use cross validation.
For this, we will use the standard number of 10 folds, and set the repetitions to 3.

```{r}
trControl <- trainControl(method = "repeatedcv", number=10, repeats=3)
```

### Training (Caret)

We will use the following models for this classification:

- KNN:
- Random Forest:
- Neural Network:
- Logistic Regression (GLM):

We select accuracy as the metric, since we have a completely balanced binary dataset we can select this metric safely (otherwise we could go for e.g. F1-score).
Variables will be normalized.

```{r, message=FALSE, results='hide'}
preProcess <- c("center","scale")
knn <- train(y ~ ., method='knn', data = training_set, metric='Accuracy',preProcess = preProcess, 
             trControl=trControl)
rf <- train(y ~ ., method='rf', data = training_set, metric='Accuracy',preProcess = preProcess, 
            trControl=trControl)
nnet <- train(y ~ ., method='nnet', data = training_set, metric='Accuracy',preProcess = preProcess, 
              trControl=trControl)
glm <- train(y ~ ., method='glm', data = training_set, metric='Accuracy',preProcess = preProcess, 
             trControl=trControl)
```
With those trained models it is possible now to make a prediction in the test partition.
```{r}
test_set$pred_knn <- predict(knn, test_set)
test_set$pred_rf <- predict(rf, test_set)
test_set$pred_nnet <- predict(nnet, test_set)
test_set$pred_glm <- predict(glm, test_set)
```

### Results (Caret)

```{r message=FALSE, warning=FALSE}
cm_knn <- confusionMatrix(test_set$y, test_set$pred_knn)
cm_rf <- confusionMatrix(test_set$y, test_set$pred_rf)
cm_nnet <- confusionMatrix(test_set$y, test_set$pred_nnet)
cm_glm <- confusionMatrix(test_set$y, test_set$pred_glm)
```

| Model | Accuracy |
| :- | -: |
| knn | `r cm_knn["overall"]$overall["Accuracy"] ` |
| rf | `r cm_rf["overall"]$overall["Accuracy"] ` |
| nnet | `r cm_nnet["overall"]$overall["Accuracy"] ` |
| glm | `r cm_glm["overall"]$overall["Accuracy"] ` |

We can see that the best performance is yielded by the random forest model.
Let's now check if an ensemble of the other 3 models can outperform this one.
For that we will just use voting and take the prediction with most votes out of the three as a result.

```{r}
votes <- test_set[, c('pred_nnet','pred_knn','pred_glm')]
votes$pred_nnet <- ifelse(votes$pred_nnet == "suburb", 1, 0)
votes$pred_knn <- ifelse(votes$pred_knn == "suburb", 1, 0)
votes$pred_glm <- ifelse(votes$pred_glm == "suburb", 1, 0)
test_set$pred_ensemble <- as.factor(ifelse(apply(votes, MARGIN=1, mean) > 0.49, "suburb", "city"))

cm_ensemble <- confusionMatrix(test_set$y, test_set$pred_ensemble)
```

| Model | (Positive) Accuracy | Recall |
| :- | -: | -: |
| knn | `r cm_knn["overall"]$overall["Accuracy"] ` |
| rf | `r cm_rf["overall"]$overall["Accuracy"] ` |
| nnet | `r cm_nnet["overall"]$overall["Accuracy"] ` |
| glm | `r cm_glm["overall"]$overall["Accuracy"] ` |
| ensemble | `r cm_ensemble["overall"]$overall["Accuracy"] ` |

It seems that the ensemble has actually a worse performance than one of the members.
This is due to us, having mixed a very good classifier (the neural network), with other two that have lower accuracy and seem to make the same errors often.
Normally, we should have tried to include diverse members in the ensemble so that they don't make the same mistakes together.

Let us see now the confusion matrix (CM) for each. First the ensemble to see it's performance.

```{r}
confusionMatrix(test_set$y, test_set$pred_ensemble)
```
As we can see in the ensemble's CM, it wrongly predicted city 17 times when it was a suburb (0.7733 accuracy), and wrongly predicted suburb 16 times when it should have been a city (0.7867 accuracy).

```{r}
confusionMatrix(test_set$y, test_set$pred_rf)
```
We see from the random forest, the accuracy was very high. But there were 8 suburb predictions when it should have been a city.

### ROC curve

```{r}
probabilities <- predict(glm, test_set, type="prob")$suburb
colAUC(probabilities, test_set$y, plotROC = TRUE)
```

### Training vs. Test Split (H2O)

Now we are going to try the same process with [H2O](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html).
In addition we will use dplyr package syntax.

```{r echo=TRUE, include=TRUE, warning=FALSE, message=FALSE}
if (!require("h2o")) install.packages("h2o")
if (!require("dplyr")) install.packages("dplyr")
library(h2o) # R interface for 'H2O', the scalable open source machine learning platform that 
# offers parallelized implementations of many supervised and unsupervised machine learning 
# algorithms such as Generalized Linear Models (GLM), Gradient Boosting Machines (including 
# XGBoost), Random Forests, Deep Neural Networks (Deep Learning), Stacked Ensembles, Naive Bayes,
# Generalized Additive Models (GAM), ANOVA GLM, Cox Proportional Hazards, KMeans, PCA, ModelSelection,
# Word2Vec, as well as a fully automatic machine learning algorithm (H2O AutoML).
library(dplyr) # A fast, consistent tool for working with data frame like objects, both in memory 
# and out of memory.
h2o.init(nthreads = -1)
```

Again we divide this dataset in a standard 70-30 split for training and testing.

```{r}
h2o_boston_work <- as.h2o(boston_work)
h2o_test_split <- h2o.splitFrame(h2o_boston_work, ratios = 0.7, seed = random_seed)
h2o_training_set <- h2o_test_split[[1]]
h2o_test_set <- h2o_test_split[[2]]
```

### Training (H2O)

We will use the following models for this classification:

- GBM:
- AutoML:

```{r, message=FALSE}
predictors <- names(h2o_boston_work)[-(length(names(h2o_boston_work)))]
response <- "y"

gbm <- h2o.gbm(x=predictors,
               y="y",
               training_frame=h2o_training_set,
               validation_frame=h2o_test_set,
               seed=random_seed)

aml <- h2o.automl(x=predictors,
               y="y",
               training_frame=h2o_training_set,
               validation_frame=h2o_test_set,
               seed=random_seed,
               max_runtime_secs = 100)
```

### Results (H2O)

```{r}
h2o.performance(gbm, h2o_test_set)
```
```{r}
aml
```

```{r}
h2o.get_best_model(aml)
```

### Conclusion

It seems that we totally can predict this neighborhood type out of the variables present in the dataset.
This exercise served as a demonstration of how to use caret and H2O to create and evaluate models for this kind of classification.

#### References

[1] https://www.kaggle.com/datasets/vikrishnan/boston-house-prices 

[2] A Boston housing dataset controversy, M Carlisle, 13 Jun 2019 <https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>

[3] J. Miguel Marin, Session-4A-Introduction-to-caret 
<https://aulaglobal.uc3m.es/course/view.php?id=156200>

[4] J. Miguel Marin, Session-4D-Introduction-to-H2O
<https://aulaglobal.uc3m.es/course/view.php?id=156200>

#### Library Citations

‘caret’:
  Kuhn M (2022). _caret: Classification and Regression Training_. R package version 6.0-93,
  <https://CRAN.R-project.org/package=caret>.

'ggplot2':
  H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New
  York, 2016.

‘dplyr’:
  Wickham H, François R, Henry L, Müller K (2022). _dplyr: A Grammar of Data Manipulation_. R package
  version 1.0.9, <https://CRAN.R-project.org/package=dplyr>.
  
‘h2o’:
  LeDell E, Gill N, Aiello S, Fu A, Candel A, Click C, Kraljevic T, Nykodym T, Aboyoun P, Kurka M,
  Malohlava M (2022). _h2o: R Interface for the 'H2O' Scalable Machine Learning Platform_. R package
  version 3.38.0.1, <https://CRAN.R-project.org/package=h2o>.
  