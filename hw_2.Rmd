---
title: "Suburbs in Boston Housing Prices"
subtitle: "HW II. Programming in R. MSc Statistics for Data Science."
author:
  - "Kendal Raymond William Smith <100494805@alumnos.uc3m.es>"
  - "Germán Blanco Blanco <100441287@alumnos.uc3m.es>"
date: '2022-10-29'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# WARNING

The dataset used in this exercise has an ethical/racist problem, as reported in [Carlisle](https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8).[2]
It is used here together with this warning, to highlight our awareness about ethical issues in data science.
Furthermore, only variables that brought no concerns will be used in this study.

## Determination of Pupil to Teach Ratio by Demographics in Boston Housing Dataset

In the [Boston Housing Dataset](https://www.kaggle.com/datasets/vikrishnan/boston-house-prices), there is one demographic variable that calls our attention.
This is the pupil to teacher ratio.

One of us lives in Paracuellos, a suburban town near Madrid, where it seems that every family has either more than two kids or several dogs.
It is interesting to see how people in need of space, but of different cultures, income levels and family structures have all gathered here.
Unsurprisingly, given the huge growth in a short period of years, one of the main problems of this town was low number of schools and teachers.
This study will look at a (otherwise controversial) dataset in the US, and try to identify the main factors for moving to such neighborhoods.

### Introduction

The goal of this work is to use several of the variables in the Boston Housing dataset and use them to classify the Pupil to Teacher ratio variable.
We can see in its histogram that the variable has a big lump around 20.
These could be the houses that belong to the group that we want to spot (suburban neighbourhoods with many pupils per teacher).

```{r}
boston <- read.csv("boston_house_prices.csv")
hist(boston$PTRATIO)
```

The median (19.05) of this variable is close to the point just below this lump around 20.
So we are going to select the rounded value of the median (19) as the point that splits the data and create a categorical variable for the classification.

The variables that we will use for input are the following:

Input features in order:  
1) ZN: proportion of residential land zoned for lots over 25,000 sq.ft.  
2) INDUS: proportion of non-retail business acres per town  
3) RM: average number of rooms per dwelling  
4) AGE: proportion of owner-occupied units built prior to 1940  
5) DIS: weighted distances to five Boston employment centres  
6) RAD: index of accessibility to radial highways  
7) TAX: full-value property-tax rate per $10,000 [$/10k]  
8) MEDV: Median value of owner-occupied homes in $1000's [k$]

Output variable:
9) PTRATIO: pupil-teacher ratio by town  

```{r}
boston_work <- boston[, c('ZN','INDUS','RM','AGE','DIS','RAD','TAX','MEDV')]
boston_work$y <- as.factor(ifelse(boston$PTRATIO > 19, "suburb", "city"))
```

### Training vs. Test Split (Caret)

We are going to use the caret package for training.
We also set random seed in order to make results reproducible.

```{r echo=TRUE, include=TRUE, warning=FALSE, message=FALSE}
if (!require("caret")) install.packages("caret")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("caTools")) install.packages("caTools")
library(caret) # Misc functions for training and plotting classification and regression models.
library(ggplot2) # ggplot2 is a system for declaratively creating graphics, based on The Grammar 
# of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical
# primitives to use, and it takes care of the details.
library(caTools) #Contains several basic utility functions including: moving (rolling,
# running) window statistic functions, read/write for GIF and ENVI binary files, fast
# calculation of AUC, LogitBoost classifier, base64 encoder/decoder, round-off-error-free 
# sumand cumsum, etc.
random_seed<-42 # the answer to everything.
set.seed(random_seed)
```

Let us start by dividing this dataset in a standard 70-30 split for training and testing.

```{r}
test_split <- createDataPartition(y = boston_work$y, p=0.7, list=FALSE)
training_set <- boston_work[test_split,]
test_set <- boston_work[-test_split,]
```

In order to tune the model parameters, we are going to use cross validation.
For this, we will use the standard number of 10 folds, and set the repetitions to 3.

```{r}
trControl <- trainControl(method = "repeatedcv", number=10, repeats=3)
```

### Training (Caret)

We will use the following models for this classification:

- KNN: the K-nearest neighbor classifier algorithm predicts the variable by finding the nearest neighbor class.[5] 
- Random Forest: The random forest algorithm is made up of many trees. It uses bagging and uses randomness when building each tree to create an uncorrelated forest whose prediction by vote is more accurate than that of an individual tree.[6]
- Neural Network: A biologically inspired algorithm that learns through analyzing data through linking different nodes (neurons)[7]
- Logistic Regression (GLM): Logistic regression measures the relationship between the dependent variable and one or more independent variables by estimating probabilities using the underlying logit function. In statistics, the logit function or the log-odds is the logarithm of the odds.[8]

We select accuracy as the metric, since we have a completely balanced binary dataset we can select this metric safely (otherwise we could go for e.g. F1-score).
Variables will be normalized.


```{r, include=FALSE}
preProcess <- c("center","scale")
knn <- train(y ~ ., method='knn', data = training_set, metric='Accuracy',preProcess = preProcess, 
             trControl=trControl)
rf <- train(y ~ ., method='rf', data = training_set, metric='Accuracy',preProcess = preProcess, 
            trControl=trControl)
nnet <- train(y ~ ., method='nnet', data = training_set, metric='Accuracy',preProcess = preProcess, 
              trControl=trControl)
glm <- train(y ~ ., method='glm', data = training_set, metric='Accuracy',preProcess = preProcess, 
             trControl=trControl)
```
With those trained models it is possible now to make a prediction in the test partition.
```{r}
test_set$pred_knn <- predict(knn, test_set)
test_set$pred_rf <- predict(rf, test_set)
test_set$pred_nnet <- predict(nnet, test_set)
test_set$pred_glm <- predict(glm, test_set)
```

### Results (Caret)

```{r message=FALSE, warning=FALSE}
cm_knn <- confusionMatrix(test_set$y, test_set$pred_knn)
cm_rf <- confusionMatrix(test_set$y, test_set$pred_rf)
cm_nnet <- confusionMatrix(test_set$y, test_set$pred_nnet)
cm_glm <- confusionMatrix(test_set$y, test_set$pred_glm)
```

| Model | Accuracy |
| :- | -: |
| knn | `r cm_knn["overall"]$overall["Accuracy"] ` |
| rf | `r cm_rf["overall"]$overall["Accuracy"] ` |
| nnet | `r cm_nnet["overall"]$overall["Accuracy"] ` |
| glm | `r cm_glm["overall"]$overall["Accuracy"] ` |

We can see that by far the best performance is yielded by the random forest model.
Let's now check if an ensemble of the other 3 models can outperform this one.
For that we will just use voting and take the prediction with most votes out of the three as a result.

```{r}

test_set$pred_ensemble <- as.factor(ifelse(test_set$pred_nnet=="suburb",
            ifelse(test_set$pred_glm=="suburb", "suburb", as.character(test_set$pred_glm)),
            ifelse(test_set$pred_glm=="suburb", as.character(test_set$pred_glm), "city")))

votes <- test_set[, c('pred_nnet','pred_knn','pred_glm')]
votes$pred_nnet <- ifelse(votes$pred_nnet == "suburb", 1, 0)
votes$pred_knn <- ifelse(votes$pred_knn == "suburb", 1, 0)
votes$pred_glm <- ifelse(votes$pred_glm == "suburb", 1, 0)
test_set$pred_ensemble <- as.factor(ifelse(apply(votes, MARGIN=1, mean) > 0.49, "suburb", "city"))


cm_ensemble <- confusionMatrix(test_set$y, test_set$pred_ensemble)
```

| Model | (Positive) Accuracy | Recall |
| :- | -: | -: |
| knn | `r cm_knn["overall"]$overall["Accuracy"] ` |
| rf | `r cm_rf["overall"]$overall["Accuracy"] ` |
| nnet | `r cm_nnet["overall"]$overall["Accuracy"] ` |
| glm | `r cm_glm["overall"]$overall["Accuracy"] ` |
| ensemble | `r cm_ensemble["overall"]$overall["Accuracy"] ` |

The ensemble has a better performance than its three members, which means it is performing its job correctly, but overall the random forest still outclasses the other three in the ensemble by a wide margin.



Let us see now the confusion matrix (CM) for each. First the ensemble to see it's performance.

```{r}
confusionMatrix(test_set$y, test_set$pred_ensemble)
```
As we can see in the ensemble's CM, it wrongly predicted city 11 times when it was a suburb (0.8533 accuracy), and wrongly predicted suburb 9 times when it should have been a city (0.8800 accuracy).
<<<<<<< HEAD

```{r}
confusionMatrix(test_set$y, test_set$pred_rf)
```
We see from the random forest, the accuracy was very high. There was only 1 city prediction when it should have been a suburb.



### Receiver Operator Characteristic (ROC) curve

A ROC curve plots the true positive rate (TPR) against the false positive rate (FPR).

```{r}
probabilities <- predict(glm, test_set, type="prob")$suburb
colAUC(probabilities, test_set$y, plotROC = TRUE, alg=c("Wilcoxon","ROC"))
```
We can get a visualisation of the performance of the model by looking at the ROC curve. A completely random result would put the curve as a line x=y (i.e. 45° line). The further from this mid-line to the top left, the better the performance of the model. We can see this model performs very well, and the value for the Area Under the Curve (AUC) is 0.912 - the overall performance of the model as a single measurement.

### Training vs. Test Split (H2O)

Now we are going to try the same process with [H2O](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html).
In addition we will use dplyr package syntax.

```{r echo=TRUE, include=TRUE, warning=FALSE, message=FALSE}
if (!require("h2o")) install.packages("h2o")
if (!require("dplyr")) install.packages("dplyr")
library(h2o) # R interface for 'H2O', the scalable open source machine learning platform that 
# offers parallelized implementations of many supervised and unsupervised machine learning 
# algorithms such as Generalized Linear Models (GLM), Gradient Boosting Machines (including 
# XGBoost), Random Forests, Deep Neural Networks (Deep Learning), Stacked Ensembles, Naive Bayes,
# Generalized Additive Models (GAM), ANOVA GLM, Cox Proportional Hazards, KMeans, PCA, ModelSelection,
# Word2Vec, as well as a fully automatic machine learning algorithm (H2O AutoML).
library(dplyr) # A fast, consistent tool for working with data frame like objects, both in memory 
# and out of memory.
h2o.init(nthreads = -1)
```

Again we divide this dataset in a standard 70-30 split for training and testing.

```{r}
h2o_boston_work <- as.h2o(boston_work)
h2o_test_split <- h2o.splitFrame(h2o_boston_work, ratios = 0.7, seed = random_seed)
h2o_training_set <- h2o_test_split[[1]]
h2o_test_set <- h2o_test_split[[2]]
```

### Training (H2O)

We will use the following models for this classification:

- GBM: Gradient Boosting Machine (for Regression and Classification) is a forward learning ensemble method. The guiding heuristic is that good predictive results can be obtained through increasingly refined approximations. H2O’s GBM sequentially builds regression trees on all the features of the dataset in a fully distributed way - each tree is built in parallel.[9]
- AutoML:H2O’s AutoML can be used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit.[10]

```{r, message=FALSE}
predictors <- names(h2o_boston_work)[-(length(names(h2o_boston_work)))]
response <- "y"

gbm <- h2o.gbm(x=predictors,
               y="y",
               training_frame=h2o_training_set,
               validation_frame=h2o_test_set,
               seed=42)

aml <- h2o.automl(x=predictors,
               y="y",
               training_frame=h2o_training_set,
               validation_frame=h2o_test_set,
               seed=42,
               max_runtime_secs = 100)
```

### Results (H2O)

```{r}
h2o.performance(gbm, h2o_test_set)
```
We can see this model has performed very well with the data set. No false negatives or false positives. 100% accuracy.

```{r}
aml
```
We see that the h2o.automl function is very interesting. It is like the ensemble we devised but perhaps even better. It has trained various models within our maximum runtime. It has ranked the models with the highest prediction accuracy. The top 10 are all very high performing models.

We can display just the best model with its statistical data below:

```{r}
h2o.get_best_model(aml)
```

### Conclusion

It seems that we totally can predict this neighborhood type out of the variables present in the dataset.
This exercise served as a demonstration of how to use caret and H2O to create and evaluate models for this kind of classification.

#### References

[1] <https://www.kaggle.com/datasets/vikrishnan/boston-house-prices>

[2] A Boston housing dataset controversy, M Carlisle, 13 Jun 2019 <https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>

[3] J. Miguel Marin, Session-4A-Introduction-to-caret 
<https://aulaglobal.uc3m.es/course/view.php?id=156200>

[4] J. Miguel Marin, Session-4D-Introduction-to-H2O
<https://aulaglobal.uc3m.es/course/view.php?id=156200>

[5] Rahul Saxena, data aspirant - Introduction to K-nearest neighbor classifier, 23 Dec 2016 <https://dataaspirant.com/k-nearest-neighbor-classifier-intro/>

[6] Tony Yiu, Towards Data Science - Understanding Random Forest, 12 Jun 2019 <https://towardsdatascience.com/understanding-random-forest-58381e0602d2#:~:text=The%20random%20forest%20is%20a,that%20of%20any%20individual%20tree.>

[7] Finstats, R-Bloggers - What’s Neural Network?, 2 Nov 2022 <https://www.r-bloggers.com/2021/11/whats-neural-network/> 

[8] ajitjaokar, Data Science Central - Explaining Logistic Regression as Generalized Linear Model (in use as a classifier), 20 Sep 2019 <https://www.datasciencecentral.com/explaining-logistic-regression-as-generalized-linear-model-in-use/#:~:text=Logistic%20Regression%20as%20GLM&text=Logistic%20regression%20measures%20the%20relationship,the%20logarithm%20of%20the%20odds.>

[9] h2o.ai - Gradient Boosting Machine (GBM), Last updated on Oct 27, 2022, <https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html>

[10] h2o.ai - AutoML: Automatic Machine Learning - Gradient Boosting Machine (GBM), Last updated on Oct 27, 2022, <https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html>

#### Library Citations

‘caret’:
  Kuhn M (2022). _caret: Classification and Regression Training_. R package version 6.0-93,
  <https://CRAN.R-project.org/package=caret>.

'ggplot2':
  H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New
  York, 2016.

‘dplyr’:
  Wickham H, François R, Henry L, Müller K (2022). _dplyr: A Grammar of Data Manipulation_. R package
  version 1.0.9, <https://CRAN.R-project.org/package=dplyr>.
  
‘h2o’:
  LeDell E, Gill N, Aiello S, Fu A, Candel A, Click C, Kraljevic T, Nykodym T, Aboyoun P, Kurka M,
  Malohlava M (2022). _h2o: R Interface for the 'H2O' Scalable Machine Learning Platform_. R package
  version 3.38.0.1, <https://CRAN.R-project.org/package=h2o>.
  
‘caTools’:
  Tuszynski J (2021). _caTools: Tools: Moving Window Statistics, GIF, Base64, ROC AUC, etc_. R package
  version 1.18.2, <https://CRAN.R-project.org/package=caTools>.
  