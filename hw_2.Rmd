---
title: "Boston Housing Prices Homework 2"
subtitle: "HW II. Programming in R. MSc Statistics for Data Science."
author:
  - "Kendal Raymond William Smith <100494805@alumnos.uc3m.es>"
  - "Germ√°n Blanco Blanco <100441287@alumnos.uc3m.es>"
date: '2022-10-29'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# WARNING

The dataset used in this exercise has an ethical/racist problem, as reported in [Carlisle](https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8).
It is used here together with this warning, to highlight our awareness about ethical issues in data science.
Furthermore, only variables that brought no concerns will be used in this study.

## Determination of Pupil to Teach Ration by Demographics in Boston Housing Dataset

In the [Boston Housing Dataset](https://www.kaggle.com/datasets/vikrishnan/boston-house-prices), there is one demographic variable that calls our attention.
This is the pupil to teacher ratio.

One of use lives in Paracuellos, a suburban town near Madrid, where it seems that every family has either more than two kids or several dogs.
It is interesting to see how people in need of space, but of different cultures, income levels and family structures have all gather here.
Unsurprisingly, given the huge growth in a short period of years, one of the main problems of this town was low number of schools and teachers.
This study will look at a (otherwise controversial) dataset in the US, and try to identify the main factors for moving to such neighborhoods.

### Introduction

The goal of this work is to use several of the variables in the Boston Housing dataset and use them to classify the Pupil to Teacher ration variable.
We can see in its histogram that the variable has a big lump around 20.
These could be the houses that belong to the group that we want to spot (suburban neigborhoods with many pupils per teacher).

```{r}
boston <- read.csv("/home/german/uc3m/Programming_R/colorandhousingprices/boston_house_prices.csv")
hist(boston$PTRATIO)
```

The median (19.05) of this variable is close to the point just below this lump around 20.
So we are going to select the rounded value of the median (19) as the point that splits the data and create a categorical variable for the classification.

```{r}
```

The variables that we will use for input are the following:

- ZN: 
- INDUS:
- RM:
- AGE:
- DIS:
- RAD:
- TAX:
- MEDV:

```{r}
boston_work <- boston[, c('ZN','INDUS','RM','AGE','DIS','RAD','TAX','MEDV')]
boston_work$y <- as.factor(ifelse(boston$PTRATIO > 19, "suburb", "city"))
```

### Training vs. Test Split (Caret)

We are going to use the caret package for training.
We also set random seed in order to make results reproducible.

```{r echo=TRUE, include=TRUE, warning=FALSE, message=FALSE}
if (!require("caret")) install.packages("caret")
if (!require("ggplot2")) install.packages("ggplot2")
library(caret)
library(ggplot2)
random_seed<-7777777
set.seed(random_seed)
```

Let us start by dividing this dataset in a standard 70-30 split for training and testing.

```{r}
test_split <- createDataPartition(y = boston_work$y, p=0.7, list=FALSE)
training_set <- boston_work[test_split,]
test_set <- boston_work[-test_split,]
```

In order to tune the model parameters, we are going to use cross validation.
For this, we will use the standard number of 10 folds, and set the repetitions to 3.

```{r}
trControl <- trainControl(method = "repeatedcv", number=10, repeats=3)
```

### Training (Caret)

We will use the following models for this classification:

- KNN:
- Random Forest:

We select accuracy as the metric, since we have a completely balanced binary dataset we can select this metric safely (otherwise we could go for e.g. F1-score).
Variables will be normalized.

```{r}
preProcess <- c("center","scale")
knn <- train(y ~ ., method='knn', data = training_set, metric='Accuracy',preProcess = preProcess, trControl=trControl)
rf <- train(y ~ ., method='rf', data = training_set, metric='Accuracy',preProcess = preProcess, trControl=trControl)
```
With those trained models it is possible now to make a prediction in the test partition.
```{r}
test_set$pred_knn <- predict(knn, test_set)
test_set$pred_rf <- predict(rf, test_set)
```

### Results (Caret)

```{r}
precision_knn <- posPredValue(test_set$y, test_set$pred_knn)
precision_rf <- posPredValue(test_set$y, test_set$pred_rf)
recall_knn <- sensitivity(test_set$y, test_set$pred_knn)
recall_rf <- sensitivity(test_set$y, test_set$pred_rf)
cm_knn <- confusionMatrix(test_set$y, test_set$pred_knn)
cm_rf <- confusionMatrix(test_set$y, test_set$pred_rf)
cm_rf$table
```

### Training vs. Test Split (H2O)

Now we are going to try the same process with [H2O](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html).
In addition we will use dplyr package syntax.

```{r echo=TRUE, include=TRUE, warning=FALSE, message=FALSE}
if (!require("h2o")) install.packages("h2o")
if (!require("dplyr")) install.packages("dplyr")
library(h2o)
library(dplyr)
h2o.init(nthreads = -1)
```

Again we divide this dataset in a standard 70-30 split for training and testing.

```{r}
h2o_boston_work <- as.h2o(boston_work)
h2o_test_split <- h2o.splitFrame(h2o_boston_work, ratios = 0.7, seed = random_seed)
h2o_training_set <- h2o_test_split[[1]]
h2o_test_set <- h2o_test_split[[2]]
```

### Training (H2O)

We will use the following models for this classification:

- GBM:
- AutoML:

```{r}
predictors <- names(h2o_boston_work)[-(length(names(h2o_boston_work)))]
response <- "y"

gbm <- h2o.gbm(x=predictors,
               y="y",
               training_frame=h2o_training_set,
               validation_frame=h2o_test_set,
               seed=random_seed)

aml <- h2o.automl(x=predictors,
               y="y",
               training_frame=h2o_training_set,
               validation_frame=h2o_test_set,
               seed=random_seed,
               max_runtime_secs = 100)
```

### Results (H2O)

```{r}
h2o.performance(gbm, h2o_test_set)
```
```{r}
aml
```

```{r}
h2o.get_best_model(aml)
```

### Conclusion

It seems that we totally can predict this neighborhood type out of the variables present in the dataset.
This exercise served as a demonstration of how to use caret and H2O to create and evaulate models for this kind of classification.

